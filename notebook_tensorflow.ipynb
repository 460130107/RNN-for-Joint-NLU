{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型介绍\n",
    "\n",
    "![](https://github.com/applenob/RNN-for-Joint-NLU/raw/master/res/arc.png)\n",
    "\n",
    "形式化表达整理：\n",
    "\n",
    "- 输入序列：$x = (x_1,...x_T)$\n",
    "- 输出序列：$y = (y_1,...y_T)$，长度和$x$相同。\n",
    "- Encoder：时刻i，\n",
    "- 隐藏状态：$h_i = [fh_i, bh_i]$，前向状态+后向状态。\n",
    "- Decoder：时刻i，\n",
    "- 状态：$s_i$，$s_i = f(s_{i-1}, y_{i-1}, h_i, c_i)$\n",
    "- 其中，context向量：$c_i$，$c_i = \\sum^{T}_{j=1}\\alpha_{i,j}h_j$\n",
    "- attention参数：$\\alpha_{i,j} = \\frac{exp(e_{i,j})}{\\sum^T_{k=1}exp(e_{i,k})}$\n",
    "- $e_{i,k} = g(s_{i-1}, h_k)$\n",
    "- $g$是一个小型神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://www.isca-speech.org/archive/Interspeech_2016/pdfs/1352.PDF\n",
    "* https://arxiv.org/pdf/1409.0473.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, w2ix):\n",
    "    idxs = list(map(lambda w: w2ix[w] if w in w2ix.keys() else w2ix[\"<UNK>\"], seq))\n",
    "    # todo\n",
    "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]  # 二维展成一维 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"dataset/atis-2.train.w-intent.iob\",\"r\").readlines()\n",
    "train = [t[:-1] for t in train]  # 去掉'\\n'\n",
    "# 数据的一行像这样：'BOS i want to fly from baltimore to dallas round trip EOS\\tO O O O O O B-fromloc.city_name O B-toloc.city_name B-round_trip I-round_trip atis_flight'\n",
    "# 分割成这样[原始句子的词，标注的序列，intent]\n",
    "train = [[t.split(\"\\t\")[0].split(\" \"),t.split(\"\\t\")[1].split(\" \")[:-1],t.split(\"\\t\")[1].split(\" \")[-1]] for t in train]\n",
    "train = [[t[0][1:-1],t[1][1:],t[2]] for t in train]  # 将BOS和EOS去掉，并去掉对应标注序列中相应的标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_in, seq_out, intent = list(zip(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(flatten(seq_in))\n",
    "slot_tag = set(flatten(seq_out))\n",
    "intent_tag = set(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"okay cost b operation prefer them reservation 100 toronto on costs c when texas seats afternoons database type please 733 497766 1145 819 730 it services bring closest phoenix following 1055 1993 along located supper flies flights friends april number 281 angeles fare sometime companies kind maximum lunch non stop are midnight different advertises pennsylvania ninth 1505 travels 12 as again next make 530 m transport for mitchell is mean 1220 listing 257 served calling denver dfw 3 equal 723 1200 york serving times quebec capacity class hours taxi taking information using orlando ap57 earlier 163 they i'm cleveland july 1024 qx i'll look 1205 four 645 pearson pm flying 615 after trans i'd 137338 choices belong like arriving sorry canada 515 qo meaning originating this 1222 1110 wednesdays continental more latest what's ticket passengers enroute 1026 thank let oh any second noon least seventeen logan trips each final twa and toward 1017 canadian find 217 mondays limo october sd 21 far 150 270 difference should includes 747 225 put stopovers inform with by doesn't 852 salt no classes of 1992 tampa instead does place 1940 various people reaches distance direct let's 727 mco currently if mornings kinds 505 want hopefully describe f departures express then city 400 tuesdays airfares options fares making lastest coach connections requesting february 4 date explain usa lands uses georgia qw has within vicinity airport 296 932 139 in you noontime 5 weekday december dl 1245 well two booking d10 755 hello year else ten returning transportation turboprop thrift long 445 771 nonstops 106 dinner ea directly tacoma great a eighteenth diego st. 405 1020 weekdays round 10 america other air nonstop 4400 daily have field stapleton departing could thereafter display j31 ap 430 visit available cheap car planes proper i've thirty rates 345 evening zone carried airline schedule plane week june late designate via sunday's takes repeating straight 1600 route 1850 815 fit inexpensive the just eastern ewr where local me 720 352 72s 269 or i sunday third 928 transcontinental 813 there coming california book capacities ac actually catch guardia interested way 1039 listings columbus order locate ord know near 420 morning newark 2 don't bur dc10 1230 train louis 323 three 110 than lowest leaving some 1700 cincinnati another ontario 343 regarding landings list limousine which minnesota los 705 somebody 82 200 would ua f28 210 seventh york's priced will d smallest include las do reaching 1000 311 one amount arrival h saturdays service 650 out shortest qualify live heading going having 279 either 845 names go stops 19 montreal 1500 atlanta rate such largest thirtieth serve limousines 1030 fifth seating 1158 travel restrictions jose reservations sort la indianapolis eye milwaukee across fourteenth it's new may 11 missouri saturday detroit itinerary lester 0900 preferably arrange fourth minneapolis 43 come airfare jet august q 8 1115 eleven baltimore begins pittsburgh 466 provides fifteenth concerning fn co tower many both can eleventh 810 during sure flight price hou used days 297 lake sixth before stopping arizona hp yn around scenario ls 1300 beginning but delta's how total land thing provided wants operating afternoon dinnertime takeoff m80 oak question offer jfk cheapest thursdays westchester that fly expensive much dallas provide yyz rent sixteen atl mia 415 get michigan boeing between reverse bound carolina layover march 1291 your here january oakland buy florida eight 757 friday those close seven county 402 1133 everywhere nevada into 3724 98 spend 1209 tuesday run serves tennessee airlines stopover still abbreviations airports anywhere sa 746 over an goes 838 at hi continent mealtime thirteenth stand paul 1 francisco also red beach 212 up fine am symbols s sundays 229 1288 wish highest less o'clock got twelfth intercontinental connects 500 united early whether 3357 trip return lufthansa business 315 prices airplane vegas repeat breakfast meals fort town connection back burbank jersey cars 934 most besides 9 wanted love help utah say 734 alaska wednesday single leave ohio that's tomorrow november charges only arrives 823 we're arrive landing looking continuing chicago nighttime 1765 runs nationair nashville 555 show yes gets connect able offers general hartfield departs tickets y today without delta see very nw all indiana us friday's last bwi schedules international home houston right september about 1100 what're seventeenth monday codes approximately tenth possible takeoffs southwest 57 leaves ground american arrangements types represented 539 eighth 1045 northwest must 305 201 6 737 abbreviation month d9s 329 416 minimum iah while charlotte laying greatest so working twenty earliest lives airplanes departure area 300 north through rental plan west arrivals carries including aircraft 630 economic now petersburg twentieth kansas same sfo 1130 cities 73s 7 philly midwest take meal connecting being name one's restriction depart boston be 825 1991 who atlanta's 130 economy trying need serviced worth nineteenth 718 american's wednesday's staying first listed numbers san tell grounds to snack sixteenth destination 2134 my dc 324 230 thursday these 2153 71 traveling use downtown rentals too world seattle philadelphia what night under aa 459 767 define cp 417 day originate start offered makes memphis once time stands ff soon colorado code washington scheduled level 2100 from ap80 55 equipment miami starting later six midway bna dollars their give dulles 80\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slot_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH=50\n",
    "sin=[]\n",
    "sout=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padding，原始序列和标注序列结尾+<EOS>+n×<PAD>\n",
    "for i in range(len(seq_in)):\n",
    "    temp = seq_in[i]\n",
    "    if len(temp)<LENGTH:\n",
    "        temp.append('<EOS>')\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    sin.append(temp)\n",
    "    \n",
    "    temp = seq_out[i]\n",
    "    if len(temp)<LENGTH:\n",
    "        while len(temp)<LENGTH:\n",
    "            temp.append('<PAD>')\n",
    "    else:\n",
    "        temp = temp[:LENGTH]\n",
    "        temp[-1]='<EOS>'\n",
    "    sout.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成word2index\n",
    "word2index = {'<PAD>': 0, '<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
    "for token in vocab:\n",
    "    if token not in word2index.keys():\n",
    "        word2index[token]=len(word2index)\n",
    "\n",
    "# 生成index2word\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "# 生成tag2index\n",
    "tag2index = {'<PAD>' : 0}\n",
    "for tag in slot_tag:\n",
    "    if tag not in tag2index.keys():\n",
    "        tag2index[tag] = len(tag2index)\n",
    "        \n",
    "# 生成index2tag\n",
    "index2tag = {v:k for k,v in tag2index.items()}\n",
    "\n",
    "# 生成intent2index\n",
    "intent2index={}\n",
    "for ii in intent_tag:\n",
    "    if ii not in intent2index.keys():\n",
    "        intent2index[ii] = len(intent2index)\n",
    "\n",
    "# 生成index2intent\n",
    "index2intent = {v:k for k,v in intent2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = list(zip(sin,sout,intent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atis_flight'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data=[]\n",
    "\n",
    "# for tr in train:\n",
    "    \n",
    "#     temp = prepare_sequence(tr[0],word2index)\n",
    "#     temp = temp.view(1,-1)\n",
    "    \n",
    "#     temp2 = prepare_sequence(tr[1],tag2index)\n",
    "#     temp2 = temp2.view(1,-1)\n",
    "    \n",
    "#     temp3 = Variable(torch.LongTensor([intent2index[tr[2]]])).cuda() if USE_CUDA else Variable(torch.LongTensor([intent2index[tr[2]]]))\n",
    "    \n",
    "#     train_data.append((temp,temp2,temp3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow的动态rnn\n",
    "\n",
    "`tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified.tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.`\n",
    "\n",
    "摘自[Whats the difference between tensorflow dynamic_rnn and rnn?](https://stackoverflow.com/questions/39734146/whats-the-difference-between-tensorflow-dynamic-rnn-and-rnn)。也就是说，静态的rnn必须提前将图展开，在执行的时候，图是固定的，并且最大长度有限制。而动态rnn可以在执行的时候，将图循环地的复用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_steps = 50\n",
    "embedding_size = 64\n",
    "hidden_size = 100\n",
    "n_layers = 2\n",
    "batch_size = 16\n",
    "vocab_size = 876\n",
    "slot_size = 120\n",
    "intent_size = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(tf.int32, [batch_size, input_steps],\n",
    "                                     name='encoder_inputs')\n",
    "# 每句输入的实际长度，除了padding\n",
    "encoder_inputs_actual_length = tf.placeholder(tf.int32, [batch_size],\n",
    "                                                   name='encoder_inputs_actual_length')\n",
    "decoder_targets = tf.placeholder(tf.int32, [batch_size, input_steps],\n",
    "                                      name='encoder_inputs')\n",
    "intent_targets = tf.placeholder(tf.int32, [batch_size],\n",
    "                                      name='intent_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size],\n",
    "                                                -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(16, 50, 64) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用单个LSTM cell\n",
    "encoder_cell = LSTMCell(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose:0' shape=(50, 16, 64) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_time_major = tf.transpose(encoder_inputs_embedded, perm=[1,0,2])\n",
    "encoder_inputs_time_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_final_state, encoder_bw_final_state) = \\\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_time_major,\n",
    "                                    sequence_length=encoder_inputs_actual_length,\n",
    "                                    dtype=tf.float32, time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(50, 16, 100) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs  # T*B*D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(50, 16, 100) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs  # T*B*D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_concat_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(16, 100) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(16, 100) dtype=float32>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state  # B*D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(16, 100) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(16, 100) dtype=float32>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state  # B*D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(hidden_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_actual_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slot_W = tf.Variable(tf.random_uniform([hidden_size*2, slot_size], -1, 1), dtype=tf.float32, name=\"slot_W\")\n",
    "slot_b = tf.Variable(tf.zeros([slot_size]), dtype=tf.float32, name=\"slot_b\")\n",
    "intent_W = tf.Variable(tf.random_uniform([hidden_size*2, intent_size], -1, 1), dtype=tf.float32, name=\"intent_W\")\n",
    "intent_b = tf.Variable(tf.zeros([intent_size]), dtype=tf.float32, name=\"intent_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求intent\n",
    "intent_logits = tf.add(tf.matmul(encoder_final_state_h, intent_W), intent_b)\n",
    "intent_prob = tf.nn.softmax(intent_logits)\n",
    "intent = tf.argmax(intent_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='SOS') * 2\n",
    "\n",
    "sos_step_embedded = tf.nn.embedding_lookup(embeddings, sos_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul:0' shape=(16,) dtype=int32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_time_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始Hack\n",
    "\n",
    "像上面Encoder使用的那样，标准的`tf.nn.dynamic_rnn`需要提前将所有的输入都提前包装到一个tensor里传过去。当Decoder需要使用上一个时间节点的输出时，这就不可能提前包装好。即标准的动态rnn相当于：$s_i = f(s_{i-1}, x_i)$；但如果这个函数的参数需要扩充，比如我们做的：$s_i = f(s_{i-1}, y_{i-1}, h_i, c_i)$。于是我们需要Hack。\n",
    "\n",
    "**Loop transition function**：\n",
    "- 关键点需要解决这个循环转移函数。\n",
    "- 这个函数是这样的映射：(time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state)。\n",
    "- 两个调用时机：1.time=0的时候调用，提供初始的cell_state和输入。2.两个时间节点之间调用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = tf.concat((sos_step_embedded, encoder_concat_outputs[0]), 1)\n",
    "    # 将上面encoder的最终state传入decoder\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    # 上一个时间节点上的输出类别，获取embedding再作为下一个时间节点的输入\n",
    "    output_logits = tf.add(tf.matmul(previous_output, slot_W), slot_b)\n",
    "    prediction = tf.argmax(output_logits, axis=1)\n",
    "    next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    # 输入是h_i+o_{i-1}\n",
    "    input_ = tf.concat((next_input, encoder_concat_outputs[time]), 1)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input_,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 16, 200) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, slot_W), slot_b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=slot_size, dtype=tf.float32),\n",
    "    logits=decoder_logits)\n",
    "\n",
    "loss_slot = tf.reduce_mean(stepwise_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(intent_targets, depth=intent_size, dtype=tf.float32),\n",
    "    logits=intent_logits)\n",
    "loss_intent = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_slot + loss_intent\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "for i, (g, v) in enumerate(grads):\n",
    "    if g is not None:\n",
    "        grads[i] = (tf.clip_by_norm(g, 5), v)  # clip gradients\n",
    "train_op = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
